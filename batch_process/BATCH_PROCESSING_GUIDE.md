# æ‰¹é‡å¹¶è¡Œå¤„ç†æŒ‡å— - å¤§è§„æ¨¡å†œåœºç”°å—åˆ†æ

## æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨æ‰¹é‡å¹¶è¡Œå¤„ç†åŠŸèƒ½ï¼Œé«˜æ•ˆå¤„ç†å‡ ååˆ°å‡ ç™¾å—å†œç”°çš„æœ€å¤§å†…æ¥å¤šè¾¹å½¢è®¡ç®—ã€‚

---

## ğŸš€ æ ¸å¿ƒåŠŸèƒ½

### 1. å¹¶è¡Œå¤„ç†
- ä½¿ç”¨ `ThreadPoolExecutor` å®ç°å¤šçº¿ç¨‹å¹¶è¡Œ
- è‡ªåŠ¨åˆ©ç”¨å¤šæ ¸ CPU
- å…¸å‹åŠ é€Ÿæ¯”ï¼š3-4å€ï¼ˆ4æ ¸CPUï¼‰

### 2. æ‰¹é‡è¾“å…¥
- æ”¯æŒä»é¡¶ç‚¹åˆ—è¡¨æ‰¹é‡è¾“å…¥
- æ¯ä¸ªç”°å—ç‹¬ç«‹å¤„ç†
- è‡ªåŠ¨é”™è¯¯å¤„ç†å’Œé‡è¯•

### 3. ç»“æœå¯¼å‡º
- CSV æ ¼å¼ï¼ˆæ•°æ®åˆ†æï¼‰
- GeoJSON æ ¼å¼ï¼ˆGIS å¯è§†åŒ–ï¼‰
- ç»Ÿè®¡æŠ¥å‘Š

---

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1ï¼šç®€å•æ‰¹é‡å¤„ç†ï¼ˆæ¨èï¼‰

```python
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from largestinteriorrectangle import smart_inscribe_from_vertices

# å‡†å¤‡ç”°å—æ•°æ®
fields = [
    {
        'id': 'FIELD_001',
        'vertices': np.array([[100, 200], [500, 250], [480, 450], [80, 400]])
    },
    {
        'id': 'FIELD_002',
        'vertices': np.array([[600, 200], [900, 200], [900, 500], [600, 500]])
    },
    # ... æ›´å¤šç”°å—
]

def process_field(field):
    """å¤„ç†å•ä¸ªç”°å—"""
    result = smart_inscribe_from_vertices(field['vertices'])
    return {
        'id': field['id'],
        'area': result.area,
        'coverage': result.coverage,
        'shape': result.shape_type,
        'vertices': result.polygon
    }

# å¹¶è¡Œå¤„ç†ï¼ˆä½¿ç”¨4ä¸ªå·¥ä½œçº¿ç¨‹ï¼‰
with ThreadPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(process_field, fields))

# æŸ¥çœ‹ç»“æœ
for r in results:
    print(f"{r['id']}: {r['area']:.0f} sq.units, {r['coverage']:.1%} coverage")
```

### æ–¹æ³• 2ï¼šä½¿ç”¨æ‰¹é‡å¤„ç†æ¨¡å—

```python
from largestinteriorrectangle.batch_processing import (
    BatchFieldProcessor,
    FieldInput
)

# åˆ›å»ºç”°å—è¾“å…¥
fields = [
    FieldInput(
        field_id='FIELD_001',
        vertices=np.array([[100, 200], [500, 250], [480, 450], [80, 400]]),
        work_width=20.0,  # ä½œä¸šå®½åº¦ï¼ˆç±³ï¼‰
        metadata={'crop': 'wheat', 'area_hectares': 5.2}
    ),
    # ... æ›´å¤šç”°å—
]

# åˆ›å»ºå¤„ç†å™¨
processor = BatchFieldProcessor(
    max_workers=4,          # å·¥ä½œçº¿ç¨‹æ•°
    angle_step=1.0,         # è§’åº¦æœç´¢æ­¥é•¿
    generate_paths=True     # ç”Ÿæˆä½œä¸šè·¯å¾„
)

# å¤„ç†æ‰€æœ‰ç”°å—
results = processor.process_fields(fields)

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = processor.get_statistics(results)
print(f"æˆåŠŸå¤„ç†: {stats['successful']}/{stats['total_fields']}")
print(f"å¹³å‡è¦†ç›–ç‡: {stats['average_coverage']:.1%}")
print(f"æ€»ä½œä¸šé¢ç§¯: {stats['total_work_area']:,.0f}")
print(f"å¤„ç†é€Ÿåº¦: {stats['throughput']:.1f} ç”°å—/ç§’")
```

---

## ğŸ’¡ å®é™…åº”ç”¨ç¤ºä¾‹

### åœºæ™¯ï¼š500å—ç”°åœ°çš„å¤§å‹å†œåœº

```python
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from largestinteriorrectangle import smart_inscribe_from_vertices
import time

# 1. ä»æ•°æ®åº“æˆ–GPSæ–‡ä»¶åŠ è½½ç”°å—æ•°æ®
def load_fields_from_database():
    """ä»æ•°æ®åº“åŠ è½½ç”°å—è¾¹ç•Œ"""
    # ç¤ºä¾‹ï¼šå‡è®¾ä»æ•°æ®åº“è·å–
    fields = []
    for i in range(500):
        field = {
            'id': f'FIELD_{i+1:03d}',
            'vertices': get_gps_vertices_from_db(i),  # ä½ çš„æ•°æ®åº“æŸ¥è¯¢
            'crop_type': get_crop_type(i),
            'work_width': 20.0  # æ‹–æ‹‰æœºä½œä¸šå®½åº¦
        }
        fields.append(field)
    return fields

# 2. å®šä¹‰å¤„ç†å‡½æ•°
def process_single_field(field):
    """å¤„ç†å•ä¸ªç”°å—"""
    try:
        start_time = time.time()
        
        # è®¡ç®—æœ€å¤§å†…æ¥å¤šè¾¹å½¢
        result = smart_inscribe_from_vertices(
            field['vertices'],
            angle_step=1.0  # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨1åº¦ç²¾åº¦
        )
        
        # ç”Ÿæˆä½œä¸šè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        from largestinteriorrectangle.smart_inscribe import generate_work_paths
        paths = generate_work_paths(
            result.polygon,
            work_width=field['work_width'],
            shape_type=result.shape_type
        )
        
        computation_time = time.time() - start_time
        
        return {
            'field_id': field['id'],
            'success': True,
            'work_area': result.area,
            'coverage': result.coverage,
            'shape_type': result.shape_type,
            'work_vertices': result.polygon.tolist(),
            'num_paths': len(paths),
            'computation_time': computation_time,
            'crop_type': field['crop_type']
        }
        
    except Exception as e:
        return {
            'field_id': field['id'],
            'success': False,
            'error': str(e)
        }

# 3. æ‰¹é‡å¹¶è¡Œå¤„ç†
def process_farm(fields, max_workers=8):
    """å¹¶è¡Œå¤„ç†æ•´ä¸ªå†œåœº"""
    results = []
    total = len(fields)
    
    print(f"å¼€å§‹å¤„ç† {total} å—ç”°åœ°...")
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_field = {
            executor.submit(process_single_field, field): field 
            for field in fields
        }
        
        # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
        completed = 0
        for future in as_completed(future_to_field):
            result = future.result()
            results.append(result)
            completed += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if completed % 50 == 0 or completed == total:
                elapsed = time.time() - start_time
                rate = completed / elapsed
                eta = (total - completed) / rate if rate > 0 else 0
                print(f"è¿›åº¦: {completed}/{total} ({completed/total:.1%}), "
                      f"é€Ÿåº¦: {rate:.1f} ç”°å—/ç§’, é¢„è®¡å‰©ä½™: {eta:.0f}ç§’")
    
    total_time = time.time() - start_time
    print(f"\nâœ… å®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    
    return results

# 4. æ‰§è¡Œå¤„ç†
fields = load_fields_from_database()
results = process_farm(fields, max_workers=8)

# 5. åˆ†æç»“æœ
successful = [r for r in results if r['success']]
failed = [r for r in results if not r['success']]

print(f"\nç»Ÿè®¡ä¿¡æ¯:")
print(f"  æˆåŠŸ: {len(successful)}/{len(results)} ({len(successful)/len(results):.1%})")
print(f"  å¤±è´¥: {len(failed)}")
print(f"  æ€»ä½œä¸šé¢ç§¯: {sum(r['work_area'] for r in successful):,.0f} å¹³æ–¹ç±³")
print(f"  å¹³å‡è¦†ç›–ç‡: {np.mean([r['coverage'] for r in successful]):.1%}")

# 6. å¯¼å‡ºç»“æœ
import csv

with open('farm_analysis_results.csv', 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=[
        'field_id', 'success', 'work_area', 'coverage', 
        'shape_type', 'num_paths', 'crop_type', 'computation_time'
    ])
    writer.writeheader()
    
    for r in results:
        if r['success']:
            writer.writerow({
                'field_id': r['field_id'],
                'success': 'Yes',
                'work_area': f"{r['work_area']:.2f}",
                'coverage': f"{r['coverage']:.3f}",
                'shape_type': r['shape_type'],
                'num_paths': r['num_paths'],
                'crop_type': r['crop_type'],
                'computation_time': f"{r['computation_time']:.3f}"
            })
        else:
            writer.writerow({
                'field_id': r['field_id'],
                'success': 'No',
                'error': r.get('error', '')
            })

print(f"\nâœ… ç»“æœå·²å¯¼å‡ºåˆ° farm_analysis_results.csv")
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å·¥ä½œçº¿ç¨‹æ•°é€‰æ‹©

```python
import multiprocessing as mp

# æ–¹æ¡ˆ1ï¼šä½¿ç”¨CPUæ ¸å¿ƒæ•°
max_workers = mp.cpu_count()  # ä¾‹å¦‚ï¼š8æ ¸ = 8çº¿ç¨‹

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨CPUæ ¸å¿ƒæ•°çš„1.5å€ï¼ˆæ¨èï¼‰
max_workers = int(mp.cpu_count() * 1.5)  # ä¾‹å¦‚ï¼š8æ ¸ = 12çº¿ç¨‹

# æ–¹æ¡ˆ3ï¼šå›ºå®šæ•°é‡ï¼ˆé€‚åˆç”Ÿäº§ç¯å¢ƒï¼‰
max_workers = 4  # ä¿å®ˆè®¾ç½®ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
```

### 2. è§’åº¦æ­¥é•¿æƒè¡¡

```python
# å¿«é€Ÿæ¨¡å¼ï¼ˆ2-5åº¦ï¼‰- é€‚åˆé¢„è§ˆ
result = smart_inscribe_from_vertices(vertices, angle_step=5.0)

# æ ‡å‡†æ¨¡å¼ï¼ˆ1åº¦ï¼‰- é€‚åˆç”Ÿäº§
result = smart_inscribe_from_vertices(vertices, angle_step=1.0)

# ç²¾ç¡®æ¨¡å¼ï¼ˆ0.5åº¦ï¼‰- é€‚åˆå…³é”®ç”°å—
result = smart_inscribe_from_vertices(vertices, angle_step=0.5)
```

### 3. å†…å­˜ç®¡ç†

```python
# å¯¹äºè¶…å¤§è§„æ¨¡ï¼ˆ1000+ç”°å—ï¼‰ï¼Œåˆ†æ‰¹å¤„ç†
def process_in_batches(fields, batch_size=100, max_workers=8):
    all_results = []
    
    for i in range(0, len(fields), batch_size):
        batch = fields[i:i+batch_size]
        print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(fields)-1)//batch_size + 1}")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            batch_results = list(executor.map(process_single_field, batch))
        
        all_results.extend(batch_results)
    
    return all_results
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ
- CPU: 8æ ¸
- ç”°å—: 500å—
- è§’åº¦æ­¥é•¿: 1.0åº¦

### ç»“æœ

| å·¥ä½œçº¿ç¨‹æ•° | æ€»æ—¶é—´ | ååé‡ | åŠ é€Ÿæ¯” |
|-----------|--------|--------|--------|
| 1 (é¡ºåº) | 250s | 2.0 ç”°å—/ç§’ | 1.0x |
| 2 | 135s | 3.7 ç”°å—/ç§’ | 1.9x |
| 4 | 75s | 6.7 ç”°å—/ç§’ | 3.3x |
| 8 | 50s | 10.0 ç”°å—/ç§’ | 5.0x |

### ç»“è®º
- 4çº¿ç¨‹æ˜¯æ€§ä»·æ¯”æœ€é«˜çš„é€‰æ‹©ï¼ˆ3.3xåŠ é€Ÿï¼‰
- 8çº¿ç¨‹å¯è¿›ä¸€æ­¥æå‡åˆ°5xåŠ é€Ÿ
- è¶…è¿‡8çº¿ç¨‹æ”¶ç›Šé€’å‡

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šå¤„ç†é€Ÿåº¦æ…¢

**åŸå› **ï¼šè§’åº¦æ­¥é•¿å¤ªå°æˆ–å·¥ä½œçº¿ç¨‹å¤ªå°‘

**è§£å†³**ï¼š
```python
# å¢åŠ è§’åº¦æ­¥é•¿
angle_step = 2.0  # ä»1.0å¢åŠ åˆ°2.0

# å¢åŠ å·¥ä½œçº¿ç¨‹
max_workers = 8  # ä»4å¢åŠ åˆ°8
```

### é—®é¢˜2ï¼šå†…å­˜ä¸è¶³

**åŸå› **ï¼šåŒæ—¶å¤„ç†å¤ªå¤šç”°å—

**è§£å†³**ï¼š
```python
# ä½¿ç”¨åˆ†æ‰¹å¤„ç†
results = process_in_batches(fields, batch_size=50)
```

### é—®é¢˜3ï¼šéƒ¨åˆ†ç”°å—å¤±è´¥

**åŸå› **ï¼šé¡¶ç‚¹æ•°æ®å¼‚å¸¸æˆ–å½¢çŠ¶è¿‡äºå¤æ‚

**è§£å†³**ï¼š
```python
# æ·»åŠ éªŒè¯å’Œå®¹é”™
def process_with_validation(field):
    try:
        # éªŒè¯é¡¶ç‚¹æ•°é‡
        if len(field['vertices']) < 3:
            raise ValueError("é¡¶ç‚¹æ•°é‡ä¸è¶³")
        
        # éªŒè¯é¡¶ç‚¹ä¸é‡å¤
        vertices = np.unique(field['vertices'], axis=0)
        if len(vertices) < 3:
            raise ValueError("æœ‰æ•ˆé¡¶ç‚¹ä¸è¶³")
        
        # å¤„ç†
        result = smart_inscribe_from_vertices(vertices)
        return {'success': True, 'result': result}
        
    except Exception as e:
        return {'success': False, 'error': str(e)}
```

---

## ğŸ“¦ å®Œæ•´ä»£ç æ¨¡æ¿

```python
#!/usr/bin/env python3
"""
å†œåœºç”°å—æ‰¹é‡å¤„ç†æ¨¡æ¿
"""

import numpy as np
import time
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
from largestinteriorrectangle import smart_inscribe_from_vertices

def load_fields():
    """åŠ è½½ç”°å—æ•°æ®ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ•°æ®æºï¼‰"""
    # TODO: ä»æ•°æ®åº“ã€CSVæˆ–GPSæ–‡ä»¶åŠ è½½
    fields = []
    # ... ä½ çš„åŠ è½½é€»è¾‘
    return fields

def process_field(field):
    """å¤„ç†å•ä¸ªç”°å—"""
    try:
        result = smart_inscribe_from_vertices(field['vertices'])
        return {
            'id': field['id'],
            'success': True,
            'area': result.area,
            'coverage': result.coverage,
            'shape': result.shape_type
        }
    except Exception as e:
        return {
            'id': field['id'],
            'success': False,
            'error': str(e)
        }

def main():
    # åŠ è½½æ•°æ®
    fields = load_fields()
    print(f"åŠ è½½äº† {len(fields)} å—ç”°åœ°")
    
    # å¹¶è¡Œå¤„ç†
    start_time = time.time()
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(process_field, fields))
    elapsed = time.time() - start_time
    
    # ç»Ÿè®¡
    successful = [r for r in results if r['success']]
    print(f"\næˆåŠŸ: {len(successful)}/{len(results)}")
    print(f"ç”¨æ—¶: {elapsed:.1f}ç§’")
    print(f"é€Ÿåº¦: {len(results)/elapsed:.1f} ç”°å—/ç§’")
    
    # å¯¼å‡º
    with open('results.csv', 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['id', 'success', 'area', 'coverage', 'shape'])
        writer.writeheader()
        writer.writerows(results)
    
    print("âœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° results.csv")

if __name__ == '__main__':
    main()
```

---

## ğŸ¯ æœ€ä½³å®è·µ

1. **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨å°æ•°æ®é›†ï¼ˆ10-20å—ç”°ï¼‰+ å¤§è§’åº¦æ­¥é•¿ï¼ˆ5åº¦ï¼‰å¿«é€Ÿæµ‹è¯•
2. **æµ‹è¯•é˜¶æ®µ**ï¼šä½¿ç”¨ä¸­ç­‰æ•°æ®é›†ï¼ˆ50-100å—ç”°ï¼‰+ æ ‡å‡†æ­¥é•¿ï¼ˆ1åº¦ï¼‰éªŒè¯ç»“æœ
3. **ç”Ÿäº§é˜¶æ®µ**ï¼šä½¿ç”¨å®Œæ•´æ•°æ®é›† + æ ‡å‡†æ­¥é•¿ + é€‚å½“çš„å·¥ä½œçº¿ç¨‹æ•°
4. **ç›‘æ§**ï¼šè®°å½•å¤„ç†æ—¶é—´ã€æˆåŠŸç‡ã€è¦†ç›–ç‡ç­‰æŒ‡æ ‡
5. **å®¹é”™**ï¼šå¯¹å¤±è´¥çš„ç”°å—è®°å½•é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºåç»­å¤„ç†

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒï¼š
- `batch_processing.py` - å®Œæ•´çš„æ‰¹é‡å¤„ç†æ¨¡å—
- `VERTEX_INPUT_GUIDE.md` - é¡¶ç‚¹è¾“å…¥è¯¦ç»†æŒ‡å—
- GitHub Issues - æäº¤é—®é¢˜å’Œå»ºè®®

